{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1193511e-93fe-44dc-abd0-3538bac13b91",
   "metadata": {
    "id": "6a728deb-28da-4064-8e66-bef416022207"
   },
   "source": [
    "## Introduction\n",
    "This script, PDF2Graph, automates the process of extracting text from PDF documents, constructing a knowledge graph, and uploading it to a Neo4j graph database. The workflow includes splitting large PDF files, parsing them to extract relevant information, creating a property graph using the extracted data, and storing the graph in Neo4j for easy querying and analysis. This tool is ideal for transforming unstructured PDF content into structured graph data for further semantic search, reasoning, or visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db8ac2-5221-44de-a53e-cb5ab37ac8f5",
   "metadata": {
    "id": "e8db8ac2-5221-44de-a53e-cb5ab37ac8f5"
   },
   "source": [
    "## Setup (Installs, Data, Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989d1cb5-5464-4d9d-ac1e-6e16276d3698",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "989d1cb5-5464-4d9d-ac1e-6e16276d3698",
    "outputId": "c584801e-f0cb-4ece-e120-9ffa638060d1"
   },
   "outputs": [],
   "source": [
    "# !pip install llama-index\n",
    "# !pip install llama-index-core==0.10.42\n",
    "# !pip install llama-index-embeddings-openai\n",
    "# !pip install llama-index-postprocessor-flag-embedding-reranker\n",
    "# !pip install git+https://github.com/FlagOpen/FlagEmbedding.git\n",
    "# !pip install llama-index-graph-stores-neo4j\n",
    "# !pip install llama-parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "580ecfc2-b082-4c4e-910a-7f88e8137aad",
   "metadata": {
    "id": "580ecfc2-b082-4c4e-910a-7f88e8137aad"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86e95e2-0fdf-4bb1-bf2e-b33af13ac7ef",
   "metadata": {
    "id": "c86e95e2-0fdf-4bb1-bf2e-b33af13ac7ef"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# API access to llama-cloud\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f683f2-a41e-4975-843c-435407132f0e",
   "metadata": {
    "id": "d6f683f2-a41e-4975-843c-435407132f0e"
   },
   "source": [
    "#### Setup Model\n",
    "\n",
    "Here we use gpt-4o and default OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91854ee-d57a-4d7b-bcc8-c6bd7214fe84",
   "metadata": {
    "id": "d91854ee-d57a-4d7b-bcc8-c6bd7214fe84"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf33f7-b195-444d-9355-ab47c91be6ad",
   "metadata": {
    "id": "5bcf33f7-b195-444d-9355-ab47c91be6ad"
   },
   "source": [
    "#### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ecs_xAO_ZkI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "id": "1ecs_xAO_ZkI",
    "outputId": "58294bea-dfd0-4215-d429-d1ea6b96e601"
   },
   "outputs": [],
   "source": [
    "pdf_path = \"D:/LLM project/test data/20190118_Rev003_Navy OH Foundation Report.pdf\"\n",
    "pdf_filename = os.path.basename(pdf_path).replace(\" \", \"\").replace(\".pdf\", \"\")\n",
    "pkl_filename = pdf_filename + \".pkl\" \n",
    "pkl_filename = os.path.join(\"split_pdfs\", pkl_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93861aba-41e0-4328-b71e-cbd455ab24cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: parsing_instruction is deprecated. Use complemental_formatting_instruction or content_guideline_instruction instead.\n",
      "Started parsing the file under job_id cdedc71b-12ad-41ac-8d11-3d44b45a9c6d\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "parsed_docs = LlamaParse(result_type=\"text\").load_data(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5bf5a38-3c81-4ebc-9058-716650e04b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PDF Splitting and Parsing Automation\n",
    "\n",
    "Description:\n",
    "This script automates the process of splitting a large PDF file into smaller parts, \n",
    "parsing each part using LlamaParse to extract textual content, and saving the parsed \n",
    "data as serialized pickle (.pkl) files. Finally, it combines all parsed segments into \n",
    "a single .pkl file for downstream use, such as semantic search, vectorization, or RAG-based applications.\n",
    "\n",
    "Dependencies:\n",
    "- PyMuPDF (fitz)\n",
    "- llama_parse\n",
    "- pickle\n",
    "- os\n",
    "\n",
    "Usage:\n",
    "Define `pdf_path`, `output_dir`, and `pkl_filename`, then run the `main()` function.\n",
    "\"\"\"\n",
    "\n",
    "import fitz \n",
    "import os\n",
    "from llama_parse import LlamaParse\n",
    "import pickle\n",
    "\n",
    "def split_pdf(pdf_path, output_dir, num_splits=5):\n",
    "   doc = fitz.open(pdf_path)\n",
    "   total_pages = len(doc)\n",
    "   pages_per_split = total_pages // num_splits\n",
    "   pdf_parts = []\n",
    "   \n",
    "   for i in range(num_splits):\n",
    "       start_page = i * pages_per_split\n",
    "       end_page = (i + 1) * pages_per_split if i < num_splits - 1 else total_pages\n",
    "       \n",
    "       split_pdf_path = os.path.join(output_dir, f\"split_{i + 1}.pdf\")\n",
    "       new_doc = fitz.open()\n",
    "       for page in range(start_page, end_page):\n",
    "           new_doc.insert_pdf(doc, from_page=page, to_page=page)\n",
    "       new_doc.save(split_pdf_path)\n",
    "       new_doc.close()\n",
    "       pdf_parts.append(split_pdf_path)\n",
    "   \n",
    "   return pdf_parts\n",
    "\n",
    "def parse_and_save_pdfs(pdf_list, output_dir):\n",
    "   parser = LlamaParse(result_type=\"text\")\n",
    "   saved_files = []\n",
    "   \n",
    "   for i, pdf in enumerate(pdf_list):\n",
    "       try:\n",
    "           docs = parser.load_data(pdf)  \n",
    "           output_file = os.path.join(output_dir, f\"parsed_{i+1}.pkl\")\n",
    "           with open(output_file, \"wb\") as f:\n",
    "               pickle.dump(docs, f)\n",
    "           saved_files.append(output_file)\n",
    "           \n",
    "       except Exception as e:\n",
    "           print(f\"Error parsing {pdf}: {e}\")\n",
    "   \n",
    "   return saved_files\n",
    "\n",
    "def combine_parsed_files(file_list, output_file):\n",
    "   all_docs = []\n",
    "   for file in file_list:\n",
    "       with open(file, \"rb\") as infile:\n",
    "           docs = pickle.load(infile)\n",
    "           all_docs.extend(docs)\n",
    "   \n",
    "   with open(output_file, \"wb\") as f:\n",
    "       pickle.dump(all_docs, f)\n",
    "\n",
    "def main(pdf_path, output_dir, pkl_filename):\n",
    "   os.makedirs(output_dir, exist_ok=True)\n",
    "   pdf_parts = split_pdf(pdf_path, output_dir, num_splits=5)\n",
    "   parsed_files = parse_and_save_pdfs(pdf_parts, output_dir)\n",
    "   combine_parsed_files(parsed_files, pkl_filename)\n",
    "\n",
    "\n",
    "output_dir = \"split_pdfs\"\n",
    "main(pdf_path, output_dir, pkl_filename)  \n",
    "\n",
    "\n",
    "with open(os.path.join(output_dir, pkl_filename), \"rb\") as f:\n",
    "   parsed_docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6078fc35-6800-4569-bd1f-8435ac7e39c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed_docs type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"parsed_docs type: {type(parsed_docs)}\")  # should be list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d17149d2-781f-4fff-85b9-7d8bca6f09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "print(pkl_filename)\n",
    "with open(pkl_filename, \"rb\") as f:\n",
    "    parsed_docs= pickle.load(f)\n",
    "\n",
    "print(parsed_docs[:50]) \n",
    "print(pkl_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9afcf38e-3c7e-4b48-ae23-61b33f1a448a",
   "metadata": {
    "id": "9afcf38e-3c7e-4b48-ae23-61b33f1a448a"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from llama_index.core.schema import TextNode, Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "def get_sub_docs(docs):\n",
    "    \"\"\"Split docs into pages, by separator.\"\"\"\n",
    "    sub_docs = []\n",
    "    for doc in docs:\n",
    "        doc_chunks = doc.text.split(\"\\n---\\n\")\n",
    "        for doc_chunk in doc_chunks:\n",
    "            sub_doc = Document(\n",
    "                text=doc_chunk,\n",
    "                metadata=deepcopy(doc.metadata),\n",
    "            )\n",
    "            sub_docs.append(sub_doc)\n",
    "\n",
    "    return sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d672fcaa-9e49-40a2-9a0b-e709351e840f",
   "metadata": {
    "id": "d672fcaa-9e49-40a2-9a0b-e709351e840f"
   },
   "outputs": [],
   "source": [
    "# this will split into pages\n",
    "sub_docs = get_sub_docs(parsed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55bcae5-43ba-4363-9109-d8080d19ce5a",
   "metadata": {
    "id": "e55bcae5-43ba-4363-9109-d8080d19ce5a"
   },
   "source": [
    "#### Initialize Graph Store\n",
    "\n",
    "Here I use Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25ed865-b78e-4856-9473-7123d9924d46",
   "metadata": {
    "id": "e25ed865-b78e-4856-9473-7123d9924d46"
   },
   "outputs": [],
   "source": [
    "from llama_index.graph_stores.neo4j import Neo4jPGStore\n",
    "password = input(\"Please enter your password for Neo4j: \")\n",
    "graph_store = Neo4jPGStore(\n",
    "    username=\"neo4j\",\n",
    "    password=password,\n",
    "    url=\"bolt://localhost:7689\",\n",
    ")\n",
    "vec_store = None\n",
    "#Built a bridge between a Python notebook and Neo4j."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10723825-328f-4175-ad85-637b0c28262c",
   "metadata": {
    "id": "10723825-328f-4175-ad85-637b0c28262c"
   },
   "source": [
    "## Construct Knowledge Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d5caf63-27ad-45e3-9643-fcc352beb69c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b3d81c4-95a8-409b-a448-ac18e193effc",
   "metadata": {
    "id": "6b3d81c4-95a8-409b-a448-ac18e193effc"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.indices.property_graph import (\n",
    "    ImplicitPathExtractor,\n",
    "    SimpleLLMPathExtractor,\n",
    ")\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.graph_stores.neo4j import Neo4jPGStore\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.core.schema import Document\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4debe49-e2a1-4092-b0e7-cc5e6604fbef",
   "metadata": {
    "id": "d4debe49-e2a1-4092-b0e7-cc5e6604fbef",
    "outputId": "7e6636c9-2bd2-4242-8561-6461bceb2857"
   },
   "outputs": [],
   "source": [
    "class MultiGraphStore:\n",
    "    def __init__(self, neo4j_config: Dict[str, str]):\n",
    "        \"\"\"\n",
    "        Constructor: Initializes the Neo4j configuration.\n",
    "        \"\"\"\n",
    "        self.neo4j_config = neo4j_config\n",
    "        self.graphs = {}\n",
    "\n",
    "    def setup_graph(self, graph_name: str):\n",
    "        \"\"\"\n",
    "        Set up the environment for each graph (using the filename as the graph name).\n",
    "        \"\"\"\n",
    "        if graph_name not in self.graphs:\n",
    "            # Create a Neo4j graph store\n",
    "            self.graphs[graph_name] = Neo4jPGStore(\n",
    "                username=self.neo4j_config[\"username\"],\n",
    "                password=self.neo4j_config[\"password\"],\n",
    "                url=self.neo4j_config[\"url\"],\n",
    "                database=\"neo4j\",  # Only use the default database\n",
    "            )\n",
    "            print(f\"Graph {graph_name} setup successfully\")\n",
    "        else:\n",
    "            print(f\"Graph {graph_name} already exists\")\n",
    "\n",
    "    def get_graph(self, graph_name: str):\n",
    "        \"\"\"\n",
    "        Retrieve the graph by its name.\n",
    "        \"\"\"\n",
    "        return self.graphs.get(graph_name)\n",
    "\n",
    "    def clear_graph(self, graph_name: str):\n",
    "        \"\"\"\n",
    "        Clear the data in the specified graph.\n",
    "        \"\"\"\n",
    "        if graph_name in self.graphs:\n",
    "            self.graphs[graph_name].clear()\n",
    "            print(f\"Graph {graph_name} cleared\")\n",
    "        else:\n",
    "            print(f\"Graph {graph_name} does not exist\")\n",
    "\n",
    "    def clear_all_graphs(self):\n",
    "        \"\"\"\n",
    "        Clear the data in all stored graphs.\n",
    "        \"\"\"\n",
    "        for graph_name in self.graphs:\n",
    "            self.graphs[graph_name].clear()\n",
    "            print(f\"Graph {graph_name} cleared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0445cfe6-f752-496d-bc43-a282e10ffdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_knowledge_graph_for_pdf(\n",
    "    file_name: str, \n",
    "    processed_docs: List[Document], \n",
    "    neo4j_config: Dict\n",
    ") -> PropertyGraphIndex:\n",
    "    \"\"\"\n",
    "    Create a knowledge graph and add it to Neo4j.\n",
    "    The PDF file name is used as the name of the knowledge graph.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the PDF file, used to name the graph.\n",
    "        processed_docs (List[Document]): List of processed documents.\n",
    "        neo4j_config (Dict): Configuration for connecting to Neo4j.\n",
    "    \"\"\"\n",
    "    # Create an instance of MultiGraphStore\n",
    "    multi_graph = MultiGraphStore(neo4j_config)\n",
    "\n",
    "    # Sanitize the file name to ensure a valid graph name\n",
    "    safe_name = \"\".join(c for c in file_name if c.isalnum() or c == '_')\n",
    "\n",
    "    # Set up the graph environment (create storage in Neo4j)\n",
    "    multi_graph.setup_graph(safe_name)\n",
    "\n",
    "    # Retrieve the corresponding graph store\n",
    "    graph_store = multi_graph.get_graph(safe_name)\n",
    "\n",
    "    # Add the graph name to each document's metadata\n",
    "    for doc in processed_docs:\n",
    "        doc.metadata = doc.metadata or {}\n",
    "        doc.metadata[\"graph_name\"] = safe_name\n",
    "\n",
    "    # Create the knowledge graph using PropertyGraphIndex\n",
    "    index = PropertyGraphIndex.from_documents(\n",
    "        processed_docs,\n",
    "        embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "        kg_extractors=[\n",
    "            ImplicitPathExtractor(),\n",
    "            SimpleLLMPathExtractor(\n",
    "                llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3),\n",
    "                num_workers=4,\n",
    "                max_paths_per_chunk=10,\n",
    "            ),\n",
    "        ],\n",
    "        property_graph_store=graph_store,\n",
    "        show_progress=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Successfully created knowledge graph for {file_name}\")\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2afc22d-9c8a-435e-945a-cf8a8bb731b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Attentionisallyouneed setup successfully\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c01c30b74d45ba82f440a04f3385f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting implicit paths: 100%|██████████| 13/13 [00:00<?, ?it/s]\n",
      "Extracting paths from text: 100%|██████████| 13/13 [00:07<00:00,  1.69it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:04<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created knowledge graph for Attentionisallyouneed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "neo4j_config = {\n",
    "    \"username\": \"neo4j\",\n",
    "    \"password\": password,\n",
    "    \"url\": \"bolt://localhost:7689\",  \n",
    "}\n",
    "\n",
    "# Create and add a knowledge graph to Neo4j\n",
    "index = create_knowledge_graph_for_pdf(pdf_filename, sub_docs, neo4j_config)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
